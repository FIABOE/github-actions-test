name: ‚ö° Performance Testing

# S'ex√©cute toutes les heures
on:
     schedule:
       - cron: '0 * * * *'  # Toutes les heures
     workflow_dispatch:

jobs:
  performance-test:
    name: üìä Test de Performance
    runs-on: ubuntu-latest
    
    steps:
      - name: üì• Checkout du code
        uses: actions/checkout@v4
      
      - name: üêç Configuration Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: üì¶ Installation des outils
        run: |
          pip install requests
      
      - name: ‚ö° Test de temps de r√©ponse
        id: response_time
        run: |
          API_URL="${{ secrets.API_URL || 'http://localhost:5000' }}"
          
          echo "‚ö° Test de temps de r√©ponse de l'API..."
          
          # Test multiple endpoints
          python3 << 'EOF'
          import requests
          import time
          import statistics
          
          api_url = "${{ secrets.API_URL || 'http://localhost:5000' }}"
          
          endpoints = [
              "/",
              "/health",
              "/metrics",
              "/api/articles"
          ]
          
          results = {}
          
          for endpoint in endpoints:
              times = []
              print(f"\nüîç Testing {endpoint}...")
              
              # Faire 5 requ√™tes pour avoir une moyenne
              for i in range(5):
                  try:
                      start = time.time()
                      response = requests.get(f"{api_url}{endpoint}", timeout=10)
                      elapsed = (time.time() - start) * 1000  # en millisecondes
                      
                      if response.status_code == 200:
                          times.append(elapsed)
                          print(f"  Request {i+1}: {elapsed:.2f}ms")
                      else:
                          print(f"  Request {i+1}: Failed (HTTP {response.status_code})")
                  except Exception as e:
                      print(f"  Request {i+1}: Error - {str(e)}")
              
              if times:
                  avg_time = statistics.mean(times)
                  min_time = min(times)
                  max_time = max(times)
                  
                  results[endpoint] = {
                      'avg': avg_time,
                      'min': min_time,
                      'max': max_time
                  }
                  
                  print(f"  ‚úÖ Average: {avg_time:.2f}ms")
                  print(f"  üìä Min: {min_time:.2f}ms, Max: {max_time:.2f}ms")
                  
                  # Alerte si trop lent (> 2000ms)
                  if avg_time > 2000:
                      print(f"  ‚ö†Ô∏è WARNING: Endpoint is slow!")
              else:
                  print(f"  ‚ùå All requests failed")
          
          # R√©sum√©
          print("\n" + "="*50)
          print("üìä PERFORMANCE SUMMARY")
          print("="*50)
          
          for endpoint, data in results.items():
              status = "üü¢" if data['avg'] < 500 else "üü°" if data['avg'] < 1000 else "üî¥"
              print(f"{status} {endpoint}: {data['avg']:.2f}ms (avg)")
          
          print("="*50)
          EOF
      
      - name: üî• Test de charge (Load Testing)
        run: |
          API_URL="${{ secrets.API_URL || 'http://localhost:5000' }}"
          
          echo "üî• Test de charge avec 50 requ√™tes concurrentes..."
          
          python3 << 'EOF'
          import requests
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed
          
          api_url = "${{ secrets.API_URL || 'http://localhost:5000' }}"
          num_requests = 50
          
          def make_request(request_id):
              try:
                  start = time.time()
                  response = requests.get(f"{api_url}/api/articles", timeout=10)
                  elapsed = time.time() - start
                  return {
                      'id': request_id,
                      'success': response.status_code == 200,
                      'time': elapsed,
                      'status': response.status_code
                  }
              except Exception as e:
                  return {
                      'id': request_id,
                      'success': False,
                      'time': 0,
                      'error': str(e)
                  }
          
          print(f"üöÄ Lancement de {num_requests} requ√™tes concurrentes...")
          start_time = time.time()
          
          with ThreadPoolExecutor(max_workers=10) as executor:
              futures = [executor.submit(make_request, i) for i in range(num_requests)]
              results = [future.result() for future in as_completed(futures)]
          
          total_time = time.time() - start_time
          
          # Analyse des r√©sultats
          successful = sum(1 for r in results if r['success'])
          failed = num_requests - successful
          avg_response_time = sum(r['time'] for r in results if r['success']) / max(successful, 1)
          
          print(f"\n{'='*50}")
          print(f"üî• LOAD TEST RESULTS")
          print(f"{'='*50}")
          print(f"‚úÖ Successful: {successful}/{num_requests} ({successful/num_requests*100:.1f}%)")
          print(f"‚ùå Failed: {failed}")
          print(f"‚ö° Avg response time: {avg_response_time*1000:.2f}ms")
          print(f"‚è±Ô∏è  Total time: {total_time:.2f}s")
          print(f"üìä Requests per second: {num_requests/total_time:.2f}")
          print(f"{'='*50}")
          
          # Alerte si taux d'√©chec > 10%
          if failed / num_requests > 0.1:
              print("‚ö†Ô∏è WARNING: High failure rate detected!")
              exit(1)
          
          # Alerte si temps de r√©ponse moyen > 2s
          if avg_response_time > 2:
              print("‚ö†Ô∏è WARNING: Slow response time detected!")
              exit(1)
          
          print("‚úÖ Load test passed!")
          EOF
      
      - name: üìä G√©n√©rer le rapport de performance
        if: always()
        run: |
          echo "================================"
          echo "üìä PERFORMANCE REPORT"
          echo "================================"
          echo "üïí Timestamp: $(date)"
          echo "üåê API URL: ${{ secrets.API_URL || 'http://localhost:5000' }}"
          echo "‚úÖ Test Result: ${{ job.status }}"
          echo "================================"
          
          # Dans un vrai projet, vous pourriez:
          # - Sauvegarder les m√©triques dans une base de donn√©es
          # - Cr√©er des graphiques d'√©volution
          # - Envoyer un rapport par email
      
      - name: üö® Alerte si performance d√©grad√©e
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const title = '‚ö†Ô∏è Performance Degradation Detected';
            const body = `
            ## ‚ö†Ô∏è Alerte: D√©gradation de Performance
            
            **Timestamp:** ${new Date().toISOString()}
            **Workflow:** ${context.workflow}
            
            Les tests de performance ont d√©tect√© des probl√®mes:
            - Temps de r√©ponse √©lev√©
            - Ou taux d'√©chec important
            
            ### Actions recommand√©es:
            - [ ] V√©rifier la charge serveur
            - [ ] Analyser les logs
            - [ ] Optimiser les requ√™tes lentes
            
            **D√©tails:** ${context.payload.repository.html_url}/actions/runs/${context.runId}
            `;
            
            // Cr√©er un issue seulement si aucun n'existe
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['performance']
            });
            
            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['performance', 'automated']
              });
            }
